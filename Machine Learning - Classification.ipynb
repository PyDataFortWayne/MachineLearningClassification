{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Pipelines](#pipelines)\n",
    "    * [Why should I use them?](#whypipelines)\n",
    "    * [Example](#pipeexample)\n",
    "* [Types of Classification](#classtypes)\n",
    "    * [Binary Classification](#binary)\n",
    "        * [Dataset for Binary Classification](#datasetbinary)\n",
    "        * [Baseline for Comparison](#baselinebinary)\n",
    "        * []()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sci-kit learn's Pipelines\n",
    "\n",
    "One useful feature that sci-kit learn has is the ability to chain together several operations that are cross-validated while setting different parameters. Each intermediate step in the pipeline is a transform, which may [clean](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing), [reduce](http://scikit-learn.org/stable/modules/unsupervised_reduction.html#data-reduction), [expand](http://scikit-learn.org/stable/modules/kernel_approximation.html#kernel-approximation), or [generate](http://scikit-learn.org/stable/modules/feature_extraction.html#feature-extraction) feature representations and must implement both `fit()` and `transform()` methods. The final step of the pipeline is an estimator, which can perform classification, regression, or clustering and only needs to call `fit()`.\n",
    "\n",
    "## Why should I use them?\n",
    "\n",
    "### Ease \n",
    "You only need to call `fit` and `predict` once on your data to fit a sequence of estimators.\n",
    "\n",
    "### Parameter selection\n",
    "You can [grid search](http://scikit-learn.org/stable/modules/grid_search.html#grid-search) over parameters of all estimators in one step.\n",
    "\n",
    "### Safety\n",
    "Because Pipelines use cross-validation, data leakage is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In this example we will generate some synthetic data using sklearn's `make_blobs()` and plot the result to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X, y = make_blobs(n_samples=200, centers = 2, n_features = 2) \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data has been generated, we can create a Pipeline to scale and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train_example, X_test_example, y_train_example, y_test_example = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "clf.fit(X_train_example, y_train_example)\n",
    "pred_test = clf.predict(X_test_example)\n",
    "\n",
    "print('{:.2%}\\n'.format(accuracy_score(y_test_example, pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy score of 90% would normally be suspect, but since we are using a very small synthetic dataset with only two features, it's not really surprising that the accuracy would be as high as it is. We can add more steps to our Pipeline without sacrificing code readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Steps to a Pipeline\n",
    "\n",
    "Since Pipelines work similarly to lists, we can add (and remove) steps from the Pipeline using the `.steps.append()` to append a step at the end of our Pipeline, or `.steps.insert()` to insert a step at a given position. If we want to remove a step, we can use the `.steps.pop(n)` to remove steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "In binary classification, the goal is to predict between two possible class labels, predicting if an e-mail is spam or ham, would be an example of binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for Binary Classification\n",
    "\n",
    "The dataset we will be using for this example is the Wisconsin Breast Cancer dataset. Like the Boston Housing Prices dataset from the regression meeting, this dataset can be loaded from sci-kit learn using `load_breast_cancer()` function from sci-kit learn's datasets package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that have the dataset loaded from sci-kit learn, let's use pandas to get more information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension   ...    worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871   ...            17.33           184.60      2019.0   \n",
       "1                 0.05667   ...            23.41           158.80      1956.0   \n",
       "2                 0.05999   ...            25.53           152.50      1709.0   \n",
       "3                 0.09744   ...            26.50            98.87       567.7   \n",
       "4                 0.05883   ...            16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cancer_df = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
    "cancer_df['target'] = cancer_data['target']\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data printed above, but that doesn't tell us very much about what the columns of data represent. Included in the data is the `DESCR` attribute, which when printed will give us a nicely formatted description of the columns of data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Wisconsin (Diagnostic) Database\n",
      "=============================================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      "References\n",
      "----------\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cancer_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    212\n",
      "1    357\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(cancer_df.groupby('target').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "\n",
    "Before we perform any cool machine learning functions with our data we need to make sure that it is split into smaller subsets, to guard against data \"leaking\" through and biasing our results. We will use sklearn's built in `train_test_split` function in `model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer_data.data, cancer_data.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Comparison\n",
    "\n",
    "Before applying any kind of pre-processing to the data, we should check what performance we are able to achieve without any additional work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation\n",
    "\n",
    "In this example we are using 10-fold cross-validation (K = 10 in this case). This will split our set into 10 separate \"pieces\", with a different \"piece\" held out during each run. More information can be found [here](http://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "model_list.append(('Tree', DecisionTreeClassifier()))\n",
    "model_list.append(('SVC', SVC()))\n",
    "model_list.append(('KNN', KNeighborsClassifier()))\n",
    "model_list.append(('NB', GaussianNB()))\n",
    "model_list.append(('LR', LogisticRegression()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree: 0.93850 (0.03878) (run time: 0.07223)\n",
      "SVC: 0.63710 (0.05079) (run time: 0.10235)\n",
      "KNN: 0.92527 (0.03131) (run time: 0.01291)\n",
      "NB: 0.94512 (0.03408) (run time: 0.00873)\n",
      "LR: 0.95169 (0.03491) (run time: 0.03902)\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in model_list:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=0)\n",
    "    start = time.time()\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print( \"{}: {:.5f} ({:.5f}) (run time: {:.5f})\".format(name, cv_results.mean(), cv_results.std(), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Dataset\n",
    "\n",
    "With our baseline established, we can now look to see how applying a scaling factor to the dataset will affect the performance of the dataset. We will use pipelines to standardize the data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledTree', Pipeline([('Scaler', StandardScaler()), ('Tree', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()), ('SVM', SVC())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledTree: 0.93420 (0.03760) (run time: 0.07306)\n",
      "ScaledSVC: 0.97589 (0.02485) (run time: 0.04525)\n",
      "ScaledKNN: 0.96705 (0.02249) (run time: 0.02362)\n",
      "ScaledNB: 0.94517 (0.03548) (run time: 0.01943)\n",
      "ScaledLR: 0.98024 (0.02497) (run time: 0.03043)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in pipelines:\n",
    "    start = time.time()\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print( '{}: {:.5f} ({:.5f}) (run time: {:.5f})'.format(name, cv_results.mean(), cv_results.std(), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning a SVM Model\n",
    "\n",
    "While we have achieved fairly high accuracy on the training set already, we can usually squeeze more performance out of a particular algorithm by tuning the hyperparameters of the algorithm. In this case we will use `GridSearchCV` and test different values of the parameter `C` along with the `kernel` hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are specifying the values of `C` and the `kernel` that we would like to iterate over when we are doing our `GridSearchCV`. We then create a Python `dict` to create a grid that can be iterated over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the `Support Vector Classifier` object and the `KFold` object, along with the `GridSearchCV` object. We will pass the scaled `X_train` dataset and the unscaled target values. We can get the best results using the `best_score_` and `best_params_` attributes of `GridSearchCV`. We can then print this, along with mean, standard deviation and parameter value from each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.98022 using {'C': 0.5, 'kernel': 'linear'}\n",
      "0.97802 (0.02186) with: {'C': 0.1, 'kernel': 'linear'}\n",
      "0.82857 (0.03920) with: {'C': 0.1, 'kernel': 'poly'}\n",
      "0.94505 (0.03113) with: {'C': 0.1, 'kernel': 'rbf'}\n",
      "0.94725 (0.02431) with: {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "0.97582 (0.02493) with: {'C': 0.3, 'kernel': 'linear'}\n",
      "0.87253 (0.04379) with: {'C': 0.3, 'kernel': 'poly'}\n",
      "0.95824 (0.02477) with: {'C': 0.3, 'kernel': 'rbf'}\n",
      "0.95385 (0.02831) with: {'C': 0.3, 'kernel': 'sigmoid'}\n",
      "0.98022 (0.02296) with: {'C': 0.5, 'kernel': 'linear'}\n",
      "0.88571 (0.03090) with: {'C': 0.5, 'kernel': 'poly'}\n",
      "0.96923 (0.02435) with: {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.95385 (0.02075) with: {'C': 0.5, 'kernel': 'sigmoid'}\n",
      "0.97363 (0.02547) with: {'C': 0.7, 'kernel': 'linear'}\n",
      "0.88571 (0.03360) with: {'C': 0.7, 'kernel': 'poly'}\n",
      "0.97363 (0.02547) with: {'C': 0.7, 'kernel': 'rbf'}\n",
      "0.95165 (0.02904) with: {'C': 0.7, 'kernel': 'sigmoid'}\n",
      "0.96923 (0.03116) with: {'C': 0.9, 'kernel': 'linear'}\n",
      "0.89231 (0.02872) with: {'C': 0.9, 'kernel': 'poly'}\n",
      "0.97143 (0.02597) with: {'C': 0.9, 'kernel': 'rbf'}\n",
      "0.94725 (0.03138) with: {'C': 0.9, 'kernel': 'sigmoid'}\n",
      "0.96923 (0.03116) with: {'C': 1.0, 'kernel': 'linear'}\n",
      "0.89231 (0.02872) with: {'C': 1.0, 'kernel': 'poly'}\n",
      "0.97582 (0.02487) with: {'C': 1.0, 'kernel': 'rbf'}\n",
      "0.94945 (0.03410) with: {'C': 1.0, 'kernel': 'sigmoid'}\n",
      "0.96923 (0.03116) with: {'C': 1.3, 'kernel': 'linear'}\n",
      "0.90110 (0.02644) with: {'C': 1.3, 'kernel': 'poly'}\n",
      "0.97582 (0.02487) with: {'C': 1.3, 'kernel': 'rbf'}\n",
      "0.93626 (0.03300) with: {'C': 1.3, 'kernel': 'sigmoid'}\n",
      "0.96923 (0.03116) with: {'C': 1.5, 'kernel': 'linear'}\n",
      "0.90769 (0.02532) with: {'C': 1.5, 'kernel': 'poly'}\n",
      "0.97802 (0.02401) with: {'C': 1.5, 'kernel': 'rbf'}\n",
      "0.93846 (0.03350) with: {'C': 1.5, 'kernel': 'sigmoid'}\n",
      "0.97143 (0.02769) with: {'C': 1.7, 'kernel': 'linear'}\n",
      "0.90769 (0.02885) with: {'C': 1.7, 'kernel': 'poly'}\n",
      "0.97802 (0.02401) with: {'C': 1.7, 'kernel': 'rbf'}\n",
      "0.93626 (0.03688) with: {'C': 1.7, 'kernel': 'sigmoid'}\n",
      "0.96923 (0.03116) with: {'C': 2.0, 'kernel': 'linear'}\n",
      "0.90989 (0.02834) with: {'C': 2.0, 'kernel': 'poly'}\n",
      "0.97802 (0.02401) with: {'C': 2.0, 'kernel': 'rbf'}\n",
      "0.93407 (0.03629) with: {'C': 2.0, 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "model = SVC()\n",
    "kfold = KFold(n_splits=num_folds, random_state=21)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, y_train)\n",
    "print(\"Best: {:.5f} using {}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"{:.5f} ({:.5f}) with: {}\".format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Our Models\n",
    "\n",
    "Now that we have improved our model's performance on the training set, we should look at how well both our baseline models and our tuned SVM model does on our testing set. Since we scaled our training data, we need to apply the same scaling to our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same algorithms for the testing that we used in establishing our baseline. Since we used default parameters for the baseline, we will use them for testing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: Tree: 0.91228\n",
      "Accuracy score: KNN: 0.95614\n",
      "Accuracy score: GaussianNB: 0.90351\n",
      "Accuracy score: LR: 0.96491\n"
     ]
    }
   ],
   "source": [
    "model_tree = DecisionTreeClassifier().fit(rescaledX, y_train)\n",
    "model_knn = KNeighborsClassifier().fit(rescaledX, y_train)\n",
    "model_NB = GaussianNB().fit(rescaledX, y_train)\n",
    "model_LR = LogisticRegression().fit(rescaledX, y_train)\n",
    "\n",
    "print('Accuracy score: Tree: {:.5f}'. format(accuracy_score(model_tree.predict(X_test_scaled), y_test)))\n",
    "print('Accuracy score: KNN: {:.5f}'. format(accuracy_score(model_knn.predict(X_test_scaled), y_test)))\n",
    "print('Accuracy score: GaussianNB: {:.5f}'. format(accuracy_score(model_NB.predict(X_test_scaled), y_test)))\n",
    "print('Accuracy score: LR: {:.5f}'. format(accuracy_score(model_LR.predict(X_test_scaled), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create our SVM model using the best values from our use of `GridSearchCV`. Using this model we can test to see how well it performs on our test data. From here we can decide to use that model or select a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.96491\n"
     ]
    }
   ],
   "source": [
    "model_svc = SVC(C= 0.5, kernel='linear')\n",
    "\n",
    "model_svc.fit(rescaledX, y_train)\n",
    "\n",
    "predictions = model_svc.predict(X_test_scaled)\n",
    "print('Accuracy score: {:.5f}'.format(accuracy_score(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "Multi-class classification, on the other hand, the goal is to predict possible class labels on more than two possible class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for Multi-Class Classification\n",
    "\n",
    "The dataset we will be using for the multi-class example is UCL Wine dataset, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine_data = load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the Binary Classification example, we will load the wine dataset into a pandas `DataFrame` so that we can get a better look at the data in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "wine_df['target'] = wine_data['target']\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, like our earlier dataset, we can use the `DESCR` attribute of the dataset to get more information about the dataset without having to look it up on an external website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine Data Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- 1) Alcohol\n",
      " \t\t- 2) Malic acid\n",
      " \t\t- 3) Ash\n",
      "\t\t- 4) Alcalinity of ash  \n",
      " \t\t- 5) Magnesium\n",
      "\t\t- 6) Total phenols\n",
      " \t\t- 7) Flavanoids\n",
      " \t\t- 8) Nonflavanoid phenols\n",
      " \t\t- 9) Proanthocyanins\n",
      "\t\t- 10)Color intensity\n",
      " \t\t- 11)Hue\n",
      " \t\t- 12)OD280/OD315 of diluted wines\n",
      " \t\t- 13)Proline\n",
      "        \t- class:\n",
      "                - class_0\n",
      "                - class_1\n",
      "                - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      "References\n",
      "----------\n",
      "(1) \n",
      "S. Aeberhard, D. Coomans and O. de Vel, \n",
      "Comparison of Classifiers in High Dimensional Settings, \n",
      "Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of \n",
      "Mathematics and Statistics, James Cook University of North Queensland. \n",
      "(Also submitted to Technometrics). \n",
      "\n",
      "The data was used with many others for comparing various \n",
      "classifiers. The classes are separable, though only RDA \n",
      "has achieved 100% correct classification. \n",
      "(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "(All results using the leave-one-out technique) \n",
      "\n",
      "(2) \n",
      "S. Aeberhard, D. Coomans and O. de Vel, \n",
      "\"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "Mathematics and Statistics, James Cook University of North Queensland. \n",
      "(Also submitted to Journal of Chemometrics). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wine_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(wine_data.data, wine_data.target, \n",
    "                                                                        test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "model_list.append(('Tree', DecisionTreeClassifier()))\n",
    "model_list.append(('SVC', SVC()))\n",
    "model_list.append(('KNN', KNeighborsClassifier()))\n",
    "model_list.append(('NB', GaussianNB()))\n",
    "model_list.append(('LR', LogisticRegression()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree: 0.86282 (0.11152) (run time: 0.01360)\n",
      "SVC: 0.42372 (0.14432) (run time: 0.01750)\n",
      "KNN: 0.65577 (0.19946) (run time: 0.01342)\n",
      "NB: 0.96026 (0.05261) (run time: 0.02149)\n",
      "LR: 0.95962 (0.04045) (run time: 0.04397)\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in model_list:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=0)\n",
    "    start = time.time()\n",
    "    cv_results = cross_val_score(model, X_train_wine, y_train_wine, cv=kfold, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print( \"{}: {:.5f} ({:.5f}) (run time: {:.5f})\".format(name, cv_results.mean(), cv_results.std(), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Dataset\n",
    "\n",
    "With our baseline established, we can now look to see how applying a scaling factor to the dataset will affect the performance of the dataset. We will use pipelines to standardize the data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledTree', Pipeline([('Scaler', StandardScaler()), ('Tree', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()), ('SVM', SVC())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledTree: 0.88782 (0.11144) (run time: 0.01776)\n",
      "ScaledSVC: 0.98397 (0.03208) (run time: 0.01841)\n",
      "ScaledKNN: 0.97692 (0.04925) (run time: 0.01782)\n",
      "ScaledNB: 0.96026 (0.05261) (run time: 0.01697)\n",
      "ScaledLR: 0.95256 (0.06223) (run time: 0.02694)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in pipelines:\n",
    "    start = time.time()\n",
    "    cv_results = cross_val_score(model, X_train_wine, y_train_wine, cv=kfold, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print( \"{}: {:.5f} ({:.5f}) (run time: {:.5f})\".format(name, cv_results.mean(), cv_results.std(), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Models\n",
    "\n",
    "Given that our models all perform reasonably well on our data and since we just did an example on tuning an SVM, we can go ahead and see how well our models perform on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: Tree: 0.92593\n",
      "Accuracy score: SVM: 1.00000\n",
      "Accuracy score: KNN: 1.00000\n",
      "Accuracy score: GaussianNB: 0.94444\n",
      "Accuracy score: LR: 1.00000\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_train_wine)\n",
    "rescaledX_wine = scaler.transform(X_train_wine)\n",
    "X_test_scaled_wine = scaler.transform(X_test_wine)\n",
    "\n",
    "model_tree = DecisionTreeClassifier().fit(rescaledX_wine, y_train_wine)\n",
    "model_knn = KNeighborsClassifier().fit(rescaledX_wine, y_train_wine)\n",
    "model_NB = GaussianNB().fit(rescaledX_wine, y_train_wine)\n",
    "model_LR = LogisticRegression().fit(rescaledX_wine, y_train_wine)\n",
    "model_SVC = SVC().fit(rescaledX_wine, y_train_wine)\n",
    "\n",
    "print('Accuracy score: Tree: {:.5f}'. format(accuracy_score(model_tree.predict(X_test_scaled_wine), y_test_wine)))\n",
    "print('Accuracy score: SVM: {:.5f}'. format(accuracy_score(model_SVC.predict(X_test_scaled_wine), y_test_wine)))\n",
    "print('Accuracy score: KNN: {:.5f}'. format(accuracy_score(model_knn.predict(X_test_scaled_wine), y_test_wine)))\n",
    "print('Accuracy score: GaussianNB: {:.5f}'. format(accuracy_score(model_NB.predict(X_test_scaled_wine), y_test_wine)))\n",
    "print('Accuracy score: LR: {:.5f}'. format(accuracy_score(model_LR.predict(X_test_scaled_wine), y_test_wine)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
