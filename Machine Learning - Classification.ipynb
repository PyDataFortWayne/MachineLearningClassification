{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Pipelines](#pipelines)\n",
    "    * [Why should I use them?](#whypipelines)\n",
    "    * [Example](#pipeexample)\n",
    "    * [Adding Steps](#addingsteps)\n",
    "* [Types of Classification](#classtypes)\n",
    "    * [Binary Classification](#binary)\n",
    "        * [Dataset for Binary Classification](#datasetbinary)\n",
    "        * [Data Density](#densitybinary)\n",
    "        * [Train Test Split](#splitbinary)\n",
    "        * [Baseline for Comparison](#baselinebinary)\n",
    "        * [K-Fold Crossvalidation](#kfold)\n",
    "        * [Description of Algorithms](#description)\n",
    "        * [Running the baseline](#runningbinary)\n",
    "        * [Scaling the dataset](#scalingbinary)\n",
    "        * [Tuning an SVM model](#tuningSVM)\n",
    "        * [Testing our Models](#testingbinary)\n",
    "    * [Multiclass Classification](#multiclass)\n",
    "        * [Dataset for Multiclass Classification](#datasetmulti)\n",
    "        * [Data Density](#densitymulti)\n",
    "        * [Train Test Split](#splitmulti)\n",
    "        * [Baseline for Comparison](#baselinemulti)\n",
    "        * [Scaling the dataset](#scalingmulti)\n",
    "        * [Running the Baseline](#runningmulti)\n",
    "        * [Testing our Models](#testingmulti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sci-kit learn's Pipelines <a class=anchor id='pipelines'>\n",
    "\n",
    "One useful feature that sci-kit learn has is the ability to chain together several operations that are cross-validated while setting different parameters. Each intermediate step in the pipeline is a transform, which may [clean](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing), [reduce](http://scikit-learn.org/stable/modules/unsupervised_reduction.html#data-reduction), [expand](http://scikit-learn.org/stable/modules/kernel_approximation.html#kernel-approximation), or [generate](http://scikit-learn.org/stable/modules/feature_extraction.html#feature-extraction) feature representations and must implement both `fit()` and `transform()` methods. The final step of the pipeline is an estimator, which can perform classification, regression, or clustering and only needs to call `fit()`.\n",
    "\n",
    "## Why should I use them? <a class=anchor id='whypipelines'>\n",
    "\n",
    "### Ease \n",
    "You only need to call `fit` and `predict` once on your data to fit a sequence of estimators.\n",
    "\n",
    "### Parameter selection\n",
    "You can [grid search](http://scikit-learn.org/stable/modules/grid_search.html#grid-search) over parameters of all estimators in one step.\n",
    "\n",
    "### Safety\n",
    "Because Pipelines use cross-validation, data leakage is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example <a class = anchor id='pipeexample'>\n",
    "\n",
    "In this example we will generate some synthetic data using sklearn's `make_blobs()` and plot the result to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X, y = make_blobs(n_samples=200, centers = 2, n_features = 2) \n",
    "df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our data has been generated, we can create a Pipeline to scale and train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train_example, X_test_example, y_train_example, y_test_example = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "clf.fit(X_train_example, y_train_example)\n",
    "pred_test = clf.predict(X_test_example)\n",
    "\n",
    "print('{:.2%}\\n'.format(accuracy_score(y_test_example, pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy score of 90% would normally be suspect, but since we are using a very small synthetic dataset with only two features, it's not really surprising that the accuracy would be as high as it is. We can add more steps to our Pipeline without sacrificing code readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Steps to a Pipeline <a class=anchor id='addingsteps'>\n",
    "\n",
    "\n",
    "Since Pipelines work similarly to lists, we can add (and remove) steps from the Pipeline using the `.steps.append()` to append a step at the end of our Pipeline, or `.steps.insert()` to insert a step at a given position. If we want to remove a step, we can use the `.steps.pop(n)` to remove steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Classification <a class = anchor id='classtypes'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification <a class=anchor id='binary'>\n",
    "\n",
    "In binary classification, the goal is to predict between two possible class labels, predicting if an e-mail is spam or ham, would be an example of binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for Binary Classification <a class=anchor id='datasetbinary'>\n",
    "\n",
    "The dataset we will be using for this example is the Wisconsin Breast Cancer dataset. Like the Boston Housing Prices dataset from the regression meeting, this dataset can be loaded from sci-kit learn using `load_breast_cancer()` function from sci-kit learn's datasets package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that have the dataset loaded from sci-kit learn, let's use pandas to get more information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cancer_df = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
    "cancer_df['target'] = cancer_data['target']\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data printed above, but that doesn't tell us very much about what the columns of data represent. Included in the data is the `DESCR` attribute, which when printed will give us a nicely formatted description of the columns of data in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer_data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer_df.groupby('target').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Density <a class=anchor id='densitybinary'>\n",
    "\n",
    "We can visualize the density of the data to get a sense of the distribution of the data. Knowing the distribution of the data can be helpful in selecting classification algorithms. This plot shows a generally Gaussian or normal distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_df.plot(kind='density', subplots=True, layout=(5,7), sharex=False, legend=False, fontsize=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split <a class=anchor id='splitbinary'>\n",
    "\n",
    "Before we perform any cool machine learning functions with our data we need to make sure that it is split into smaller subsets, to guard against data \"leaking\" through and biasing our results. We will use sklearn's built in `train_test_split` function in `model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer_data.data, cancer_data.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Comparison <a class=anchor id='baselinebinary'>\n",
    "\n",
    "Before applying any kind of pre-processing to the data, we should check what performance we are able to achieve without any additional work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation <a class=anchor id='kfold'>\n",
    "\n",
    "In this example we are using 10-fold cross-validation (K = 10 in this case). This will split our set into 10 separate \"pieces\", with a different \"piece\" held out during each run. More information can be found [here](http://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description of Algorithms <a class=anchor id='description'>\n",
    "\n",
    "##### K-Nearest Neighbors: \n",
    "\n",
    "In KNN, class membership is determined by examining the class membership of the K (default is 5 in sklearn) closest points to the data point in question. More information can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) and [here](http://scikit-learn.org/stable/modules/neighbors.html).\n",
    "\n",
    "##### Support Vector Classifier:\n",
    "\n",
    "Support Vector Machines seek to make data linearly separable, so that classification decisions can be made. The use of different kernel functions allows the SVM flexibility when different shapes of data are present. More information can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) and [here](http://scikit-learn.org/stable/modules/svm.html). \n",
    "\n",
    "##### Decision Tree:\n",
    "\n",
    "Decision Trees work by learning simple decision rules inferred from the data features. Each feature is split into yes/no questions for values above or below a certain value or presence/absence of a given feature. More information can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) or [here](http://scikit-learn.org/stable/modules/tree.html).\n",
    "\n",
    "##### Gaussian Naive Bayes:\n",
    "\n",
    "Gaussian Naive Bayes works by applying [Bayes Theorem](https://plato.stanford.edu/entries/bayes-theorem/) with strong (naive) assumptions about feature independence. We are using the Gaussian version since our data has a Gaussian distribution. More information can be found [here](http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes). \n",
    "\n",
    "##### Logistic Regression: \n",
    "\n",
    "Logistic Regression, despite having regression in the name, is a linear model for classification. The equation for binary classification takes the form $\\hat{y} = w[0] * x[0] + w[1] * x[1] + ... + w[n] * x[n] + b > 0$, if $\\hat{y}$ is greater than 0 we predict the class +1, else -1. More information can be found [here](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "model_list.append(('Tree', DecisionTreeClassifier()))\n",
    "model_list.append(('SVC', SVC()))\n",
    "model_list.append(('KNN', KNeighborsClassifier()))\n",
    "model_list.append(('NB', GaussianNB()))\n",
    "model_list.append(('LR', LogisticRegression()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Baseline <a class=anchor id='runningbinary'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in model_list:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=0)\n",
    "    start = time.time()\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print( \"{}: {:.5f} ({:.5f}) (run time: {:.5f})\".format(name, cv_results.mean(), cv_results.std(), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Dataset <a class=anchor id='scalingbinary'>\n",
    "\n",
    "With our baseline established, we can now look to see how applying a scaling factor to the dataset will affect the performance of the dataset. We will use pipelines to standardize the data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledTree', Pipeline([('Scaler', StandardScaler()), ('Tree', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()), ('SVM', SVC())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in pipelines:\n",
    "    start = time.time()\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print( '{}: {:.5f} ({:.5f}) (run time: {:.5f})'.format(name, cv_results.mean(), cv_results.std(), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning a SVM Model <a class=anchor id='tuningSVM'>\n",
    "\n",
    "While we have achieved fairly high accuracy on the training set already, we can usually squeeze more performance out of a particular algorithm by tuning the hyperparameters of the algorithm. In this case we will use `GridSearchCV` and test different values of the parameter `C` along with the `kernel` hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are specifying the values of `C` and the `kernel` that we would like to iterate over when we are doing our `GridSearchCV`. We then create a Python `dict` to create a grid that can be iterated over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the `Support Vector Classifier` object and the `KFold` object, along with the `GridSearchCV` object. We will pass the scaled `X_train` dataset and the unscaled target values. We can get the best results using the `best_score_` and `best_params_` attributes of `GridSearchCV`. We can then print this, along with mean, standard deviation and parameter value from each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC()\n",
    "kfold = KFold(n_splits=num_folds, random_state=21)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, y_train)\n",
    "print(\"Best: {:.5f} using {}\".format(grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"{:.5f} ({:.5f}) with: {}\".format(mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Our Models <a class=anchor id='testingbinary'>\n",
    "\n",
    "Now that we have improved our model's performance on the training set, we should look at how well both our baseline models and our tuned SVM model does on our testing set. Since we scaled our training data, we need to apply the same scaling to our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same algorithms for the testing that we used in establishing our baseline. Since we used default parameters for the baseline, we will use them for testing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tree = DecisionTreeClassifier().fit(rescaledX, y_train)\n",
    "model_knn = KNeighborsClassifier().fit(rescaledX, y_train)\n",
    "model_NB = GaussianNB().fit(rescaledX, y_train)\n",
    "model_LR = LogisticRegression().fit(rescaledX, y_train)\n",
    "\n",
    "print('Accuracy score: Tree: {:.5f}'. format(accuracy_score(model_tree.predict(X_test_scaled), y_test)))\n",
    "print('Accuracy score: KNN: {:.5f}'. format(accuracy_score(model_knn.predict(X_test_scaled), y_test)))\n",
    "print('Accuracy score: GaussianNB: {:.5f}'. format(accuracy_score(model_NB.predict(X_test_scaled), y_test)))\n",
    "print('Accuracy score: LR: {:.5f}'. format(accuracy_score(model_LR.predict(X_test_scaled), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create our SVM model using the best values from our use of `GridSearchCV`. Using this model we can test to see how well it performs on our test data. From here we can decide to use that model or select a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svc = SVC(C= 0.5, kernel='linear')\n",
    "\n",
    "model_svc.fit(rescaledX, y_train)\n",
    "\n",
    "predictions = model_svc.predict(X_test_scaled)\n",
    "print('Accuracy score: {:.5f}'.format(accuracy_score(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification <a class=anchor id='multiclass'>\n",
    "\n",
    "Multiclass classification, on the other hand, the goal is to predict possible class labels on more than two possible class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for Multiclass Classification <a class=anchor id='datasetmulti'>\n",
    "\n",
    "The dataset we will be using for the multiclass example is UCL Wine dataset, the goal of the dataset is to predict which cultivar a sample of wine is from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine_data = load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the Binary Classification example, we will load the wine dataset into a pandas `DataFrame` so that we can get a better look at the data in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "wine_df['target'] = wine_data['target']\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, like our earlier dataset, we can use the `DESCR` attribute of the dataset to get more information about the dataset without having to look it up on an external website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Density <a class=anchor id='densitymulti'>\n",
    "\n",
    "We can visualize the density of the data to get a sense of the distribution of the data. Knowing the distribution of the data can be helpful in selecting classification algorithms. This plot shows a generally Gaussian or normal distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.plot(kind='density', subplots=True, layout=(5,7), sharex=False, legend=False, fontsize=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split <a class=anchor id='splitmulti'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(wine_data.data, wine_data.target, \n",
    "                                                                        test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline for Comparison <a class=anchor id='baselinemulti'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import KFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "model_list.append(('Tree', DecisionTreeClassifier()))\n",
    "model_list.append(('SVC', SVC()))\n",
    "model_list.append(('KNN', KNeighborsClassifier()))\n",
    "model_list.append(('NB', GaussianNB()))\n",
    "model_list.append(('LR', LogisticRegression()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Baseline <a class=anchor id='runningmulti'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in model_list:\n",
    "    kfold = KFold(n_splits=num_folds, random_state=0)\n",
    "    start = time.time()\n",
    "    cv_results = cross_val_score(model, X_train_wine, y_train_wine, cv=kfold, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print( \"{}: {:.5f} ({:.5f}) (run time: {:.5f})\".format(name, cv_results.mean(), cv_results.std(), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the Dataset <a class=anchor id='scalingmulti'>\n",
    "\n",
    "With our baseline established, we can now look to see how applying a scaling factor to the dataset will affect the performance of the dataset. We will use pipelines to standardize the data and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledTree', Pipeline([('Scaler', StandardScaler()), ('Tree', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledSVC', Pipeline([('Scaler', StandardScaler()), ('SVM', SVC())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in pipelines:\n",
    "    start = time.time()\n",
    "    cv_results = cross_val_score(model, X_train_wine, y_train_wine, cv=kfold, scoring='accuracy')\n",
    "    end = time.time()\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print( \"{}: {:.5f} ({:.5f}) (run time: {:.5f})\".format(name, cv_results.mean(), cv_results.std(), end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our Models <a class=anchor id='testingmulti'>\n",
    "\n",
    "Given that our models all perform reasonably well on our data and since we just did an example on tuning an SVM, we can go ahead and see how well our models perform on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train_wine)\n",
    "rescaledX_wine = scaler.transform(X_train_wine)\n",
    "X_test_scaled_wine = scaler.transform(X_test_wine)\n",
    "\n",
    "model_tree = DecisionTreeClassifier().fit(rescaledX_wine, y_train_wine)\n",
    "model_knn = KNeighborsClassifier().fit(rescaledX_wine, y_train_wine)\n",
    "model_NB = GaussianNB().fit(rescaledX_wine, y_train_wine)\n",
    "model_LR = LogisticRegression().fit(rescaledX_wine, y_train_wine)\n",
    "model_SVC = SVC().fit(rescaledX_wine, y_train_wine)\n",
    "\n",
    "print('Accuracy score: Tree: {:.5f}'. format(accuracy_score(model_tree.predict(X_test_scaled_wine), y_test_wine)))\n",
    "print('Accuracy score: SVM: {:.5f}'. format(accuracy_score(model_SVC.predict(X_test_scaled_wine), y_test_wine)))\n",
    "print('Accuracy score: KNN: {:.5f}'. format(accuracy_score(model_knn.predict(X_test_scaled_wine), y_test_wine)))\n",
    "print('Accuracy score: GaussianNB: {:.5f}'. format(accuracy_score(model_NB.predict(X_test_scaled_wine), y_test_wine)))\n",
    "print('Accuracy score: LR: {:.5f}'. format(accuracy_score(model_LR.predict(X_test_scaled_wine), y_test_wine)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
